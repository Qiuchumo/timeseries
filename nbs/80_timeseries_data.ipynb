{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.basics import *\n",
    "# from fastai2.data.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.data.external import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from zipfile import ZipFile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Data\n",
    "> Basic functions to read timeseries files like `.arff` and `.ts` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@docs\n",
    "class TSData():\n",
    "    \"Class that loads .arff (soon .ts) files and returns a tuple (data.x , self.y)\"\n",
    "    \"self.x is a list of 2D array with a shape (n_samples, nb_channels, sequence_length) \"\n",
    "    \"self.y is a 1D array as y (i.e. label) with a shape (n_samples)\"\n",
    "    \"for the NATOPS_Train.arff file, the result will be : x(180, 24, 51) and y(180)\"\n",
    "    # def __init__(self):\n",
    "    #     self.x = self.y = self.dsname = self.fnames = [],[],[],[]\n",
    "    \n",
    "    def __init__(self, fnames, has_targets=True, fill_missing='NaN'):\n",
    "        # self.x = self.y = self.dsname = [],[],[]\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.dsname = []\n",
    "        self.fnames = fnames\n",
    "        self.has_targets = has_targets\n",
    "        self.fill_missings = fill_missing\n",
    "    \n",
    "    def __repr__(self): return f\"{self.__class__.__name__}:\\n Datasets names (concatenated): {self.dsname}\\n Filenames:                     {self.fnames}\\n Data shape: {self.x.shape}\\n Targets shape: {self.y.shape}\\n Nb Samples: {self.x.shape[0]}\\n Nb Channels:           {self.x.shape[1]}\\n Sequence Length: {self.x.shape[2]}\"\n",
    "    \n",
    "    def get_x(self, as_list=True): return(list(self.x))\n",
    "    def get_y(self): return(self.y)\n",
    "    def get_items(self): return [(item, str(label)) for (item, label) in zip(list(self.x), self.y)]\n",
    "    def __getitem__(self, i): return (self.x[i], str(self.y[i]))\n",
    "\n",
    "    @property\n",
    "    def sizes(self): return (self.x.shape, self.y.shape)\n",
    "    \n",
    "    @property\n",
    "    def n_channels(self): return (self.x.shape[1])\n",
    "    \n",
    "    def _load_arff(self, fname, has_targets=True, fill_missing='NaN'):\n",
    "        \"load an .arff file and return a tupple of 2 numpy arrays: \"\n",
    "        \"x : array with a shape (n_samples, nb_channels, sequence_length)\"\n",
    "        \"y : array with a shape (n_samples)\"\n",
    "        \"for the NATOPS_Train.arff  the result will be : x(180, 24, 51) and y(180)\"\n",
    "        \n",
    "        instance_list = []\n",
    "        class_val_list = []\n",
    "        data_started = False\n",
    "        is_multi_variate = False\n",
    "        is_first_case = True\n",
    "        \n",
    "        with open(fname, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    if is_multi_variate is False and \"@attribute\" in line.lower() and \"relational\" in line.lower():\n",
    "                        is_multi_variate = True\n",
    "                    if \"@data\" in line.lower():\n",
    "                        data_started = True\n",
    "                        continue\n",
    "                    # if the 'data tag has been found, the header information has been cleared and now data can be loaded\n",
    "                    if data_started:\n",
    "                        line = line.replace(\"?\", fill_missing)\n",
    "                        if is_multi_variate:\n",
    "                            if has_targets:\n",
    "                                line, class_val = line.split(\"',\")\n",
    "                                class_val_list.append(class_val.strip())\n",
    "                            dimensions = line.split(\"\\\\n\")\n",
    "                            dimensions[0] = dimensions[0].replace(\"'\", \"\")\n",
    "\n",
    "                            if is_first_case:\n",
    "                                for d in range(len(dimensions)):\n",
    "                                    instance_list.append([])\n",
    "                                is_first_case = False\n",
    "\n",
    "                            for dim in range(len(dimensions)):\n",
    "                                instance_list[dim].append(np.array(dimensions[dim].split(','), dtype=np.float32))\n",
    "#                                 instance_list[dim].append(np.fromiter(dimensions[dim].split(','), dtype=np.float32))\n",
    "                        else:\n",
    "                            if is_first_case:\n",
    "                                instance_list.append([])\n",
    "                                is_first_case = False\n",
    "\n",
    "                            line_parts = line.split(\",\")\n",
    "\n",
    "                            if has_targets:\n",
    "                                instance_list[0].append(np.array(line_parts[:len(line_parts)-1], dtype=np.float32))\n",
    "\n",
    "                                class_val_list.append(line_parts[-1].strip())\n",
    "                            else:\n",
    "                                instance_list[0].append(np.array(line_parts[:len(line_parts)-1], dtype=np.float32))\n",
    "\n",
    "        #instance_list has a shape of (dimensions, nb_samples, seq_lenght)\n",
    "        #for the NATOPS_Train.arff it would be (24, 180, 51)\n",
    "        #convert python list to numpy array and transpose the 2 first dimensions -> (180, 24, 51)\n",
    "        x = np.asarray(instance_list).transpose(1,0,2) \n",
    "        \n",
    "        if has_targets:\n",
    "            y = np.asarray(class_val_list)       \n",
    "            return x, y\n",
    "        else:\n",
    "            return x, [None*x.shape[0]]\n",
    "\n",
    "    @classmethod\n",
    "    def from_arff(self, fnames, has_targets=True, fill_missing='NaN'):\n",
    "        \"load an .arff file and return a tupple of 2 numpy arrays: \"\n",
    "        \"x : array with a shape (n_samples, nb_channels, sequence_length)\"\n",
    "        \"y : array with a shape (n_samples)\"\n",
    "        \"for the NATOPS_Train.arff  the result will be : x(180, 24, 51) and y(180)\"\n",
    "        data = self(fnames, has_targets=has_targets, fill_missing=fill_missing)\n",
    "        if isinstance(fnames, list):\n",
    "            data.x = []\n",
    "            data.y = []\n",
    "            data.dsname = []\n",
    "            data.fnames = []\n",
    "            xs,ys = [],[]\n",
    "            for i, fn in enumerate(fnames):\n",
    "                x,y = data._load_arff(fn, has_targets=has_targets, fill_missing=fill_missing)\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "                data.fnames.append(fn)\n",
    "                data.dsname.append(fn.stem)\n",
    "            data.x = np.concatenate(xs)\n",
    "            data.y = np.concatenate(ys)\n",
    "        else:\n",
    "            data.fnames.append(fnames)\n",
    "            data.dsname.append(fnames.stem)\n",
    "            data.x, data.y = data._load(fnames, has_targets=has_targets, fill_missing=fill_missing)\n",
    "\n",
    "        return data\n",
    "\n",
    "# add_docs(TSData,\n",
    "#          from_arff=\"read one or serveral arff files and concatenate them, and returns a TSData object\")\n",
    "\n",
    "    _docs=dict(\n",
    "         from_arff=\"read one or serveral arff files and concatenate them, and returns a TSData object\",\n",
    "         get_items=\"return list of tuples. Each tuple corresponds to a timeserie (nump.ndarray) and a label (string)\",\n",
    "         get_x=\"return list of timeseries (no labels)\",\n",
    "         get_y=\"return list of labels corresponding to each timeserie\",\n",
    "         sizes=\"return timeseries shape and labels shape (labels list size)\",\n",
    "         n_channels=\"return timeserie's number of channels. For `arff` files it is called `dimension`. In the case of NATOPS_Train.arff, it returns 24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"TSData.from_arff\" class=\"doc_header\"><code>TSData.from_arff</code><a href=\"__main__.py#L96\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TSData.from_arff</code>(**`fnames`**, **`has_targets`**=*`True`*, **`fill_missing`**=*`'NaN'`*)\n\nread one or serveral arff files and concatenate them, and returns a TSData object",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TSData.from_arff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"TSData.get_items\" class=\"doc_header\"><code>TSData.get_items</code><a href=\"__main__.py#L24\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>TSData.get_items</code>()\n\nreturn list of tuples. Each tuple corresponds to a timeserie (nump.ndarray) and a label (string)",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TSData.get_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"TSData.n_channels\" class=\"doc_header\"><code>TSData.n_channels</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\nreturn timeserie's number of channels. For `arff` files it is called `dimension`. In the case of NATOPS_Train.arff, it returns 24",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(TSData.n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_ts_items(fnames):\n",
    "    'get_ts_items return list of tuples. Each tuple corresponds to a timeserie (nump.ndarray) and a label (string)'\n",
    "    data = TSData.from_arff(fnames)\n",
    "    return data.get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"get_ts_items\" class=\"doc_header\"><code>get_ts_items</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>get_ts_items</code>(**`fnames`**)\n\nget_ts_items return list of tuples. Each tuple corresponds to a timeserie (nump.ndarray) and a label (string)",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ts_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"get_ts_items\" class=\"doc_header\"><code>get_ts_items</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>get_ts_items</code>(**`fnames`**)\n\nget_ts_items return list of tuples. Each tuple corresponds to a timeserie (nump.ndarray) and a label (string)",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ts_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ext(fnames, ext):\n",
    "        if isinstance(fnames, list):\n",
    "            fnames = [fn if (fn.suffix!='') else f'{fn}.{ext}' for fn in fnames] \n",
    "        else:\n",
    "            fnames = fnames if (fnames.suffix!='') else f'{fnames}.{ext}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_timeseries(ts, ctx=None, title=None, chs=None, leg=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Plot a timeseries.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        title : usually the class of the timeseries \n",
    "\n",
    "        ts : timeseries. It should have a shape of (nb_channels, sequence_length)\n",
    "\n",
    "        chs : array representing a list of channels to plot \n",
    "\n",
    "        leg : Display or not a legend\n",
    "    \"\"\"\n",
    "\n",
    "    if ctx is None: fig, ctx = plt.subplots()\n",
    "    t = range(ts.shape[1])\n",
    "    chs_max = max(chs) if chs else 0\n",
    "    channels = chs if (chs and (chs_max < ts.shape[0])) else range(ts.shape[0]) \n",
    "    for ch in channels:\n",
    "        ctx.plot(t, ts[ch], label='ch'+str(ch))\n",
    "    if leg: ctx.legend(loc='upper right', ncol=2, framealpha=0.5)\n",
    "    if title: ctx.set_title(title)\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"show_timeseries\" class=\"doc_header\"><code>show_timeseries</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>show_timeseries</code>(**`ts`**, **`ctx`**=*`None`*, **`title`**=*`None`*, **`chs`**=*`None`*, **`leg`**=*`True`*, **\\*\\*`kwargs`**)\n\nPlot a timeseries.\n\nArgs:\n\n    title : usually the class of the timeseries \n\n    ts : timeseries. It should have a shape of (nb_channels, sequence_length)\n\n    chs : array representing a list of channels to plot \n\n    leg : Display or not a legend",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(show_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Path('/home/farid/.fastai/data')"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data = Config().data\n",
    "path_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def file_extract_at_filename(fname, dest):\n",
    "    \"Extract `fname` to `dest`/`fname`.name folder using `tarfile` or `zipfile\" \n",
    "    dest = Path(dest)/Path(fname).with_suffix('').name\n",
    "    # tarfile.open(fname, 'r:gz').extractall(dest)\n",
    "    fname = str(fname)\n",
    "    if   fname.endswith('gz'):  tarfile.open(fname, 'r:gz').extractall(dest)\n",
    "    elif fname.endswith('zip'): zipfile.ZipFile(fname     ).extractall(dest)\n",
    "    else: raise Exception(f'Unrecognized archive: {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file_extract_at_filename` is used by default in `unzip_data` to decompress the downloaded file in a folder that has the same name as the zip filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def unzip_data(url, fname=None, dest=None, c_key='data', force_download=False):\n",
    "    \"Download `url` to `fname` if `dest` doesn't exist, and un-compress to `dest`/`fname`.name folder .\"\n",
    "    return untar_data(url, fname=fname, c_key=c_key, force_download=force_download, extract_func=file_extract_at_filename)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unzip_data` download and decompress the downloaded file in a folder and decompress it in a folder that has the same name as the zip filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"unzip_data\" class=\"doc_header\"><code>unzip_data</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>unzip_data</code>(**`url`**, **`fname`**=*`None`*, **`dest`**=*`None`*, **`c_key`**=*`'data'`*, **`force_download`**=*`False`*)\n\nDownload `url` to `fname` if `dest` doesn't exist, and un-compress to `dest`/`fname`.name folder .",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(unzip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class URLs_TS():\n",
    "    \"Global constants for dataset and model URLs.\"\n",
    "    LOCAL_PATH = Path.cwd()\n",
    "    URL = 'http://www.timeseriesclassification.com/Downloads/'\n",
    "\n",
    "\n",
    "    # UCR multivariate datasets - Current Naming\n",
    "    ARTICULARY_WORD_RECOGNITION   = f'{URL}ArticularyWordRecognition.zip'\n",
    "    ATRIAL_FIBRILLATION           = f'{URL}AtrialFibrillation.zip'\n",
    "    BASIC_MOTIONS                 = f'{URL}BasicMotions.zip'\n",
    "    CHARACTER_TRAJECTORIES        = f'{URL}CharacterTrajectories.zip'\n",
    "    CRICKET                       = f'{URL}Cricket.zip'\n",
    "    DUCK_DUCK_GEESE               = f'{URL}DuckDuckGeese.zip'\n",
    "    EIGEN_WORMS                   = f'{URL}EigenWorms.zip'\n",
    "    EPILEPSY                      = f'{URL}Epilepsy.zip'\n",
    "    ETHANOL_CONCENTRATION         = f'{URL}EthanolConcentration.zip'\n",
    "    ERING                        = f'{URL}ERing.zip'\n",
    "    FACE_DETECTION                = f'{URL}FaceDetection.zip'\n",
    "    FINGER_MOVEMENTS              = f'{URL}FingerMovements.zip'\n",
    "    HAND_MOVEMENT_DIRECTION       = f'{URL}HandMovementDirection.zip'\n",
    "    HANDWRITING                   = f'{URL}Handwriting.zip'\n",
    "    HEARTBEAT                     = f'{URL}Heartbeat.zip'\n",
    "    JAPANESE_VOWELS               = f'{URL}JapaneseVowels.zip'\n",
    "    LIBRAS                        = f'{URL}Libras.zip'\n",
    "    LSST                          = f'{URL}LSST.zip'\n",
    "    INSECT_WINGBEAT               = f'{URL}InsectWingbeat.zip'\n",
    "    MOTOR_IMAGERY                 = f'{URL}MotorImagery.zip'\n",
    "    NATOPS                        = f'{URL}NATOPS.zip'\n",
    "    PEN_DIGITS                    = f'{URL}PenDigits.zip'\n",
    "    PEMS_SF                       = f'{URL}PEMS-SF.zip'\n",
    "    PHONEME_SPECTRA               = f'{URL}PhonemeSpectra.zip'\n",
    "    RACKET_SPORTS                 = f'{URL}RacketSports.zip'\n",
    "    SELF_REGULATION_SCP1          = f'{URL}SelfRegulationSCP1.zip'\n",
    "    SELF_REGULATION_SCP2          = f'{URL}SelfRegulationSCP2.zip'\n",
    "    SPOKEN_ARABIC_DIGITS          = f'{URL}SpokenArabicDigits.zip'\n",
    "    STAND_WALK_JUMP               = f'{URL}StandWalkJump.zip'\n",
    "    UWAVE_GESTURE_LIBRARY         = f'{URL}UWaveGestureLibrary.zip'\n",
    "\n",
    "    # UCR multivariate datasets - New Naming\n",
    "    # MULTI_ARTICULARY_WORD_RECOGNITION   = f'{URL}ArticularyWordRecognition.zip'\n",
    "    # MULTI_ATRIAL_FIBRILLATION           = f'{URL}AtrialFibrillation.zip'\n",
    "    # MULTI_BASIC_MOTIONS                 = f'{URL}BasicMotions.zip'\n",
    "    # MULTI_CHARACTER_TRAJECTORIES        = f'{URL}CharacterTrajectories.zip'\n",
    "    # MULTI_CRICKET                       = f'{URL}Cricket.zip'\n",
    "    # MULTI_DUCK_DUCK_GEESE               = f'{URL}DuckDuckGeese.zip'\n",
    "    # MULTI_EIGEN_WORMS                   = f'{URL}EigenWorms.zip'\n",
    "    # MULTI_EPILEPSY                      = f'{URL}Epilepsy.zip'\n",
    "    # MULTI_ETHANOL_CONCENTRATION         = f'{URL}EthanolConcentration.zip'\n",
    "    # MULTI_ERING                         = f'{URL}ERing.zip'\n",
    "    # MULTI_FACE_DETECTION                = f'{URL}FaceDetection.zip'\n",
    "    # MULTI_FINGER_MOVEMENTS              = f'{URL}FingerMovements.zip'\n",
    "    # MULTI_HAND_MOVEMENT_DIRECTION       = f'{URL}HandMovementDirection.zip'\n",
    "    # MULTI_HANDWRITING                   = f'{URL}Handwriting.zip'\n",
    "    # MULTI_HEARTBEAT                     = f'{URL}Heartbeat.zip'\n",
    "    # MULTI_JAPANESE_VOWELS               = f'{URL}JapaneseVowels.zip'\n",
    "    # MULTI_LIBRAS                        = f'{URL}Libras.zip'\n",
    "    # MULTI_LSST                          = f'{URL}LSST.zip'\n",
    "    # MULTI_INSECT_WINGBEAT               = f'{URL}InsectWingbeat.zip'\n",
    "    # MULTI_MOTOR_IMAGERY                 = f'{URL}MotorImagery.zip'\n",
    "    # MULTI_NATOPS                        = f'{URL}NATOPS.zip'\n",
    "    # MULTI_PEN_DIGITS                    = f'{URL}PenDigits.zip'\n",
    "    # MULTI_PEMS_SF                       = f'{URL}PEMS-SF.zip'\n",
    "    # MULTI_PHONEME_SPECTRA               = f'{URL}PhonemeSpectra.zip'\n",
    "    # MULTI_RACKET_SPORTS                 = f'{URL}RacketSports.zip'\n",
    "    # MULTI_SELF_REGULATION_SCP1          = f'{URL}SelfRegulationSCP1.zip'\n",
    "    # MULTI_SELF_REGULATION_SCP2          = f'{URL}SelfRegulationSCP2.zip'\n",
    "    # MULTI_SPOKEN_ARABIC_DIGITS          = f'{URL}SpokenArabicDigits.zip'\n",
    "    # MULTI_STAND_WALK_JUMP               = f'{URL}StandWalkJump.zip'\n",
    "    # MULTI_U_WAVE_GESTURE_LIBRARY        = f'{URL}UWaveGestureLibrary'\n",
    "\n",
    "    # UCR univariate datasets\n",
    "    UNI_ACSF1                           = f'{URL}ACSF1.zip'\n",
    "    UNI_ADIAC                           = f'{URL}Adiac.zip'\n",
    "    UNI_ALL_GESTURE_WIIMOTE_X           = f'{URL}AllGestureWiimoteX.zip'\n",
    "    UNI_ALL_GESTURE_WIIMOTE_Y           = f'{URL}AllGestureWiimoteY.zip'\n",
    "    UNI_ALL_GESTURE_WIIMOTE_Z           = f'{URL}AllGestureWiimoteZ.zip'\n",
    "    UNI_ARROW_HEAD                      = f'{URL}ArrowHead.zip'\n",
    "    UNI_BEEF                            = f'{URL}Beef.zip'\n",
    "    UNI_BEETLE_FLY                      = f'{URL}BeetleFly.zip'\n",
    "    UNI_BIRD_CHICKEN                    = f'{URL}BirdChicken.zip'\n",
    "    UNI_BME                             = f'{URL}BME.zip'\n",
    "    UNI_CAR                             = f'{URL}Car.zip'\n",
    "    UNI_CBF                             = f'{URL}CBF.zip'\n",
    "    UNI_CHINATOWN                       = f'{URL}Chinatown.zip'\n",
    "    UNI_CHLORINE_CONCENTRATION          = f'{URL}ChlorineConcentration.zip'\n",
    "    UNI_CIN_CEC_GTORSO                  = f'{URL}CinCECGtorso.zip'\n",
    "    UNI_COFFEE                          = f'{URL}Coffee.zip'\n",
    "    UNI_COMPUTERS                       = f'{URL}Computers.zip'\n",
    "    UNI_CRICKET_X                       = f'{URL}CricketX.zip'\n",
    "    UNI_CRICKET_Y                       = f'{URL}CricketY.zip'\n",
    "    UNI_CRICKET_Z                       = f'{URL}CricketZ.zip'\n",
    "    UNI_CROP                            = f'{URL}Crop.zip'\n",
    "    UNI_DIATOM_SIZE_REDUCTION           = f'{URL}DiatomSizeReduction.zip'\n",
    "    UNI_DISTAL_PHALANX_OUTLINE_AGE_GROUP= f'{URL}DistalPhalanxOutlineAgeGroup.zip'\n",
    "    UNI_DISTAL_PHALANX_OUTLINE_CORRECT  = f'{URL}DistalPhalanxOutlineCorrect.zip'\n",
    "    UNI_DISTAL_PHALANX_TW               = f'{URL}DistalPhalanxTW.zip'\n",
    "    UNI_DODGER_LOOP_DAY                 = f'{URL}DodgerLoopDay.zip'\n",
    "    UNI_DODGER_LOOP_GAME                = f'{URL}DodgerLoopGame.zip'\n",
    "    UNI_DODGER_LOOP_WEEKEND             = f'{URL}DodgerLoopWeekend.zip'\n",
    "    UNI_EARTHQUAKES                     = f'{URL}Earthquakes.zip'\n",
    "    UNI_ECG200                          = f'{URL}ECG200.zip'\n",
    "    UNI_ECG5000                         = f'{URL}ECG5000.zip'\n",
    "    UNI_ECG_FIVE_DAYS                   = f'{URL}ECGFiveDays.zip'\n",
    "    UNI_ELECTRIC_DEVICES                = f'{URL}ElectricDevices.zip'\n",
    "    UNI_EOG_HORIZONTAL_SIGNAL           = f'{URL}EOGHorizontalSignal.zip'\n",
    "    UNI_EOG_VERTICAL_SIGNAL             = f'{URL}EOGVerticalSignal.zip'\n",
    "    UNI_ETHANOL_LEVEL                   = f'{URL}EthanolLevel.zip'\n",
    "    UNI_FACE_ALL                        = f'{URL}FaceAll.zip'\n",
    "    UNI_FACE_FOUR                       = f'{URL}FaceFour.zip'\n",
    "    UNI_FACES_UCR                       = f'{URL}FacesUCR.zip'\n",
    "    UNI_FIFTY_WORDS                     = f'{URL}FiftyWords.zip'\n",
    "    UNI_FISH                            = f'{URL}Fish.zip'\n",
    "    UNI_FORD_A                          = f'{URL}FordA.zip'\n",
    "    UNI_FORD_B                          = f'{URL}FordB.zip'\n",
    "    UNI_FREEZER_REGULAR_TRAIN           = f'{URL}FreezerRegularTrain.zip'\n",
    "    UNI_FREEZER_SMALL_TRAIN             = f'{URL}FreezerSmallTrain.zip'\n",
    "    UNI_FUNGI                           = f'{URL}Fungi.zip'\n",
    "    UNI_GESTURE_MID_AIR_D1              = f'{URL}GestureMidAirD1.zip'\n",
    "    UNI_GESTURE_MID_AIR_D2              = f'{URL}GestureMidAirD2.zip'\n",
    "    UNI_GESTURE_MID_AIR_D3              = f'{URL}GestureMidAirD3.zip'\n",
    "    UNI_GESTURE_PEBBLE_Z1               = f'{URL}GesturePebbleZ1.zip'\n",
    "    UNI_GESTURE_PEBBLE_Z2               = f'{URL}GesturePebbleZ2.zip'\n",
    "    UNI_GUN_POINT                       = f'{URL}GunPoint.zip'\n",
    "    UNI_GUN_POINT_AGE_SPAN              = f'{URL}GunPointAgeSpan.zip'\n",
    "    UNI_GUN_POINT_MALE_VERSUS_FEMALE    = f'{URL}GunPointMaleVersusFemale.zip'\n",
    "    UNI_GUN_POINT_OLD_VERSUS_YOUNG      = f'{URL}GunPointOldVersusYoung.zip'\n",
    "    UNI_HAM                             = f'{URL}Ham.zip'\n",
    "    UNI_HAND_OUTLINES                   = f'{URL}HandOutlines.zip'\n",
    "    UNI_HAPTICS                         = f'{URL}Haptics.zip'\n",
    "    UNI_HERRING                         = f'{URL}Herring.zip'\n",
    "    UNI_HOUSE_TWENTY                    = f'{URL}HouseTwenty.zip'\n",
    "    UNI_INLINE_SKATE                    = f'{URL}InlineSkate.zip'\n",
    "    UNI_INSECT_EPG_REGULAR_TRAIN        = f'{URL}InsectEPGRegularTrain.zip'\n",
    "    UNI_INSECT_EPG_SMALL_TRAIN          = f'{URL}InsectEPGSmallTrain.zip'\n",
    "    UNI_INSECT_WINGBEAT_SOUND           = f'{URL}InsectWingbeatSound.zip'\n",
    "    UNI_ITALY_POWER_DEMAND              = f'{URL}ItalyPowerDemand.zip'\n",
    "    UNI_LARGE_KITCHEN_APPLIANCES        = f'{URL}LargeKitchenAppliances.zip'\n",
    "    UNI_LIGHTNING2                      = f'{URL}Lightning2.zip'\n",
    "    UNI_LIGHTNING7                      = f'{URL}Lightning7.zip'\n",
    "    UNI_MALLAT                          = f'{URL}Mallat.zip'\n",
    "    UNI_MEAT                            = f'{URL}Meat.zip'\n",
    "    UNI_MEDICAL_IMAGES                  = f'{URL}MedicalImages.zip'\n",
    "    UNI_MELBOURNE_PEDESTRIAN            = f'{URL}MelbournePedestrian.zip'\n",
    "    UNI_MIDDLE_PHALANX_OUTLINE_AGE_GROUP= f'{URL}MiddlePhalanxOutlineAgeGroup.zip'\n",
    "    UNI_MIDDLE_PHALANX_OUTLINE_CORRECT  = f'{URL}MiddlePhalanxOutlineCorrect.zip'\n",
    "    UNI_MIDDLE_PHALANX_TW               = f'{URL}MiddlePhalanxTW.zip'\n",
    "    UNI_MIXED_SHAPES                    = f'{URL}MixedShapes.zip'\n",
    "    UNI_MIXED_SHAPES_SMALL_TRAIN        = f'{URL}MixedShapesSmallTrain.zip'\n",
    "    UNI_MOTE_STRAIN                     = f'{URL}MoteStrain.zip'\n",
    "    UNI_NON_INVASIVE_FETAL_ECG_THORAX1  = f'{URL}NonInvasiveFetalECGThorax1.zip'\n",
    "    UNI_NON_INVASIVE_FETAL_ECG_THORAX2  = f'{URL}NonInvasiveFetalECGThorax2.zip'\n",
    "    UNI_OLIVE_OIL                       = f'{URL}OliveOil.zip'\n",
    "    UNI_OSU_LEAF                        = f'{URL}OSULeaf.zip'\n",
    "    UNI_PHALANGES_OUTLINES_CORRECT      = f'{URL}PhalangesOutlinesCorrect.zip'\n",
    "    UNI_PHONEME                         = f'{URL}Phoneme.zip'\n",
    "    UNI_PICKUP_GESTURE_WIIMOTE_Z        = f'{URL}PickupGestureWiimoteZ.zip'\n",
    "    UNI_PIG_AIRWAY_PRESSURE             = f'{URL}PigAirwayPressure.zip'\n",
    "    UNI_PIG_ART_PRESSURE                = f'{URL}PigArtPressure.zip'\n",
    "    UNI_PIG_CVP                         = f'{URL}PigCVP.zip'\n",
    "    UNI_PLAID                           = f'{URL}PLAID.zip'\n",
    "    UNI_PLANE                           = f'{URL}Plane.zip'\n",
    "    UNI_POWER_CONS                      = f'{URL}PowerCons.zip'\n",
    "    UNI_PROXIMAL_PHALANX_OUTLINE_AGE_GROUP= f'{URL}ProximalPhalanxOutlineAgeGroup.zip'\n",
    "    UNI_PROXIMAL_PHALANX_OUTLINE_CORRECT= f'{URL}ProximalPhalanxOutlineCorrect.zip'\n",
    "    UNI_PROXIMAL_PHALANX_TW             = f'{URL}ProximalPhalanxTW.zip'\n",
    "    UNI_REFRIGERATION_DEVICES           = f'{URL}RefrigerationDevices.zip'\n",
    "    UNI_ROCK                            = f'{URL}Rock.zip'\n",
    "    UNI_SCREEN_TYPE                     = f'{URL}ScreenType.zip'\n",
    "    UNI_SEMG_HAND_GENDER_CH2            = f'{URL}SemgHandGenderCh2.zip'\n",
    "    UNI_SEMG_HAND_MOVEMENT_CH2          = f'{URL}SemgHandMovementCh2.zip'\n",
    "    UNI_SEMG_HAND_SUBJECT_CH2           = f'{URL}SemgHandSubjectCh2.zip'\n",
    "    UNI_SHAKE_GESTURE_WIIMOTE_Z         = f'{URL}ShakeGestureWiimoteZ.zip'\n",
    "    UNI_SHAPELET_SIM                    = f'{URL}ShapeletSim.zip'\n",
    "    UNI_SHAPES_ALL                      = f'{URL}ShapesAll.zip'\n",
    "    UNI_SMALL_KITCHEN_APPLIANCES        = f'{URL}SmallKitchenAppliances.zip'\n",
    "    UNI_SMOOTH_SUBSPACE                 = f'{URL}SmoothSubspace.zip'\n",
    "    UNI_SONY_AIBO_ROBOT_SURFACE1        = f'{URL}SonyAIBORobotSurface1.zip'\n",
    "    UNI_SONY_AIBO_ROBOT_SURFACE2        = f'{URL}SonyAIBORobotSurface2.zip'\n",
    "    UNI_STARLIGHT_CURVES                = f'{URL}StarlightCurves.zip'\n",
    "    UNI_STRAWBERRY                      = f'{URL}Strawberry.zip'\n",
    "    UNI_SWEDISH_LEAF                    = f'{URL}SwedishLeaf.zip'\n",
    "    UNI_SYMBOLS                         = f'{URL}Symbols.zip'\n",
    "    UNI_SYNTHETIC_CONTROL               = f'{URL}SyntheticControl.zip'\n",
    "    UNI_TOE_SEGMENTATION1               = f'{URL}ToeSegmentation1.zip'\n",
    "    UNI_TOE_SEGMENTATION2               = f'{URL}ToeSegmentation2.zip'\n",
    "    UNI_TRACE                           = f'{URL}Trace.zip'\n",
    "    UNI_TWO_LEAD_ECG                    = f'{URL}TwoLeadECG.zip'\n",
    "    UNI_TWO_PATTERNS                    = f'{URL}TwoPatterns.zip'\n",
    "    UNI_UMD                             = f'{URL}UMD.zip'\n",
    "    UNI_U_WAVE_GESTURE_LIBRARY_ALL      = f'{URL}UWaveGestureLibraryAll.zip'\n",
    "    UNI_U_WAVE_GESTURE_LIBRARY_X        = f'{URL}UWaveGestureLibraryX.zip'\n",
    "    UNI_U_WAVE_GESTURE_LIBRARY_Y        = f'{URL}UWaveGestureLibraryY.zip'\n",
    "    UNI_U_WAVE_GESTURE_LIBRARY_Z        = f'{URL}UWaveGestureLibraryZ.zip'\n",
    "    UNI_WAFER                           = f'{URL}Wafer.zip'\n",
    "    UNI_WINE                            = f'{URL}Wine.zip'\n",
    "    UNI_WORD_SYNONYMS                   = f'{URL}WordSynonyms.zip'\n",
    "    UNI_WORMS                           = f'{URL}Worms.zip'\n",
    "    UNI_WORMS_TWO_CLASS                 = f'{URL}WormsTwoClass.zip'\n",
    "    UNI_YOGA                            = f'{URL}Yoga.zip'\n",
    "\n",
    "    def path(url='.', c_key='archive'):\n",
    "        fname = url.split('/')[-1]\n",
    "        local_path = URLs.LOCAL_PATH/('models' if c_key=='models' else 'data')/fname\n",
    "        if local_path.exists(): return local_path\n",
    "        return Config()[c_key]/fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsname =  'NATOPS' #'NATOPS', 'LSST', 'Wine', 'Epilepsy', 'HandMovementDirection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Path('/home/farid/.fastai/data/NATOPS')"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = unzip_data(URLs_TS.NATOPS)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(#54) [Path('/home/farid/.fastai/data/NATOPS/NATOPS.jpg'),Path('/home/farid/.fastai/data/NATOPS/NATOPS.txt'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension10_TEST.arff'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension10_TRAIN.arff'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension11_TEST.arff'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension11_TRAIN.arff'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension12_TEST.arff'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension12_TRAIN.arff'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension13_TEST.arff'),Path('/home/farid/.fastai/data/NATOPS/NATOPSDimension13_TRAIN.arff')...]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Path('/home/farid/.fastai/data/NATOPS/NATOPS_TRAIN.arff'),\n Path('/home/farid/.fastai/data/NATOPS/NATOPS_TEST.arff')]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname_train = f'{dsname}_TRAIN.arff'\n",
    "fname_test = f'{dsname}_TEST.arff'\n",
    "fnames = [path/fname_train, path/fname_test]\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "TSData:\n Datasets names (concatenated): ['NATOPS_TRAIN', 'NATOPS_TEST']\n Filenames:                     [Path('/home/farid/.fastai/data/NATOPS/NATOPS_TRAIN.arff'), Path('/home/farid/.fastai/data/NATOPS/NATOPS_TEST.arff')]\n Data shape: (360, 24, 51)\n Targets shape: (360,)\n Nb Samples: 360\n Nb Channels:           24\n Sequence Length: 51"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TSData.from_arff(fnames)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "TSData:\n Datasets names (concatenated): ['NATOPS_TRAIN', 'NATOPS_TEST']\n Filenames:                     [Path('/home/farid/.fastai/data/NATOPS/NATOPS_TRAIN.arff'), Path('/home/farid/.fastai/data/NATOPS/NATOPS_TEST.arff')]\n Data shape: (360, 24, 51)\n Targets shape: (360,)\n Nb Samples: 360\n Nb Channels:           24\n Sequence Length: 51\n"
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(['NATOPS_TRAIN', 'NATOPS_TEST'],\n [Path('/home/farid/.fastai/data/NATOPS/NATOPS_TRAIN.arff'),\n  Path('/home/farid/.fastai/data/NATOPS/NATOPS_TEST.arff')],\n 24,\n ((360, 24, 51), (360,)),\n (360, 24, 51),\n (360,))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dsname, data.fnames, data.n_channels, data.sizes, data.x.shape, data.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(data.dsname, ['NATOPS_TRAIN', 'NATOPS_TEST'])\n",
    "test_eq(data.n_channels, 24)\n",
    "test_eq(data.sizes, ((360, 24, 51), (360,)))\n",
    "test_eq(data.x.shape, (360, 24, 51))\n",
    "test_eq(data.y.shape, (360,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(numpy.ndarray,\n array([[-0.54737 , -0.546334, -0.549748, ..., -0.533726, -0.528338,\n         -0.518618],\n        [-1.600105, -1.599419, -1.595734, ..., -1.576063, -1.572246,\n         -1.565955],\n        [-0.809446, -0.80942 , -0.812398, ..., -0.766209, -0.764902,\n         -0.765835],\n        ...,\n        [ 0.618919,  0.648665,  0.618913, ...,  0.455396,  0.457002,\n          0.456688],\n        [-1.497652, -1.465919, -1.50323 , ..., -1.435609, -1.422537,\n         -1.421817],\n        [-0.754927, -0.706829, -0.758939, ..., -0.538306, -0.530174,\n         -0.529384]], dtype=float32))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.get_items()[1][0]), data.get_items()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(numpy.str_, '3.0')"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.get_y()[1]), data.get_y()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(data.get_y()[1], '3.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 4\n",
    "# ts, title = train_x[idx], train_y[idx]\n",
    "# show_timeseries(ts, title=title, chs=range(0,24,3)) \n",
    "# # show_timeseries(ts, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_timeseries(ts, title=title, chs=[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read '.ts' file into 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_tsfile_to_array(full_file_path_and_name, return_separate_X_and_y=True, replace_missing_vals_with='NaN'):\n",
    "    \"\"\"Loads data from a .ts file into a Pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    full_file_path_and_name: str\n",
    "        The full pathname of the .ts file to read.\n",
    "    return_separate_X_and_y: bool\n",
    "        true if X and Y values should be returned as separate Data Frames (X) and a numpy array (y), false otherwise.\n",
    "        This is only relevant for data that\n",
    "    replace_missing_vals_with: str\n",
    "       The value that missing values in the text file should be replaced with prior to parsing.\n",
    "\n",
    "    Returns\n",
    "    DataFrame, ndarray\n",
    "        If return_separate_X_and_y then a tuple containing a DataFrame and a numpy array containing the relevant time-series and            corresponding class values.\n",
    "    DataFrame\n",
    "        If not return_separate_X_and_y then a single DataFrame containing all time-series and (if relevant) a column \"class_vals\"           the associated class values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize flags and variables used when parsing the file\n",
    "\n",
    "    metadata_started = False\n",
    "    data_started = False\n",
    "\n",
    "    has_problem_name_tag = False\n",
    "    has_timestamps_tag = False\n",
    "    has_univariate_tag = False\n",
    "    has_class_labels_tag = False\n",
    "    has_data_tag = False\n",
    "\n",
    "    previous_timestamp_was_int = None\n",
    "    previous_timestamp_was_timestamp = None\n",
    "    num_dimensions = None\n",
    "    is_first_case = True\n",
    "    instance_list = []\n",
    "    class_val_list = []\n",
    "    line_num = 0\n",
    "\n",
    "    # Parse the file\n",
    "    # print(full_file_path_and_name)\n",
    "    with open(full_file_path_and_name, 'r',encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line and change to lowercase for use below\n",
    "            line = line.strip().lower()\n",
    "            # Empty lines are valid at any point in a file\n",
    "            if line:\n",
    "                # Check if this line contains metadata\n",
    "                # Please note that even though metadata is stored in this function it is not currently published externally\n",
    "                if line.startswith(\"@problemname\"):\n",
    "                    # Check that the data has not started\n",
    "                    if data_started:\n",
    "                        raise TsFileParseException(\"metadata must come before data\")\n",
    "                    # Check that the associated value is valid\n",
    "                    tokens = line.split(' ')\n",
    "                    token_len = len(tokens)\n",
    "\n",
    "                    if token_len == 1:\n",
    "                        raise TsFileParseException(\"problemname tag requires an associated value\")\n",
    "\n",
    "                    problem_name = line[len(\"@problemname\") + 1:]\n",
    "                    has_problem_name_tag = True\n",
    "                    metadata_started = True\n",
    "\n",
    "                elif line.startswith(\"@timestamps\"):\n",
    "\n",
    "                    # Check that the data has not started\n",
    "\n",
    "                    if data_started:\n",
    "                        raise TsFileParseException(\"metadata must come before data\")\n",
    "\n",
    "                    # Check that the associated value is valid\n",
    "\n",
    "                    tokens = line.split(' ')\n",
    "                    token_len = len(tokens)\n",
    "\n",
    "                    if token_len != 2:\n",
    "                        raise TsFileParseException(\"timestamps tag requires an associated Boolean value\")\n",
    "\n",
    "                    elif tokens[1] == \"true\":\n",
    "                        timestamps = True\n",
    "\n",
    "                    elif tokens[1] == \"false\":\n",
    "                        timestamps = False\n",
    "\n",
    "                    else:\n",
    "                        raise TsFileParseException(\"invalid timestamps value\")\n",
    "\n",
    "                    has_timestamps_tag = True\n",
    "                    metadata_started = True\n",
    "\n",
    "                elif line.startswith(\"@univariate\"):\n",
    "\n",
    "                    # Check that the data has not started\n",
    "\n",
    "                    if data_started:\n",
    "                        raise TsFileParseException(\"metadata must come before data\")\n",
    "\n",
    "                    # Check that the associated value is valid\n",
    "\n",
    "                    tokens = line.split(' ')\n",
    "                    token_len = len(tokens)\n",
    "\n",
    "                    if token_len != 2:\n",
    "                        raise TsFileParseException(\"univariate tag requires an associated Boolean value\")\n",
    "\n",
    "                    elif tokens[1] == \"true\":\n",
    "                        univariate = True\n",
    "\n",
    "                    elif tokens[1] == \"false\":\n",
    "                        univariate = False\n",
    "\n",
    "                    else:\n",
    "                        raise TsFileParseException(\"invalid univariate value\")\n",
    "\n",
    "                    has_univariate_tag = True\n",
    "                    metadata_started = True\n",
    "\n",
    "                elif line.startswith(\"@classlabel\"):\n",
    "\n",
    "                    # Check that the data has not started\n",
    "\n",
    "                    if data_started:\n",
    "                        raise TsFileParseException(\"metadata must come before data\")\n",
    "\n",
    "                    # Check that the associated value is valid\n",
    "\n",
    "                    tokens = line.split(' ')\n",
    "                    token_len = len(tokens)\n",
    "\n",
    "                    if token_len == 1:\n",
    "                        raise TsFileParseException(\"classlabel tag requires an associated Boolean value\")\n",
    "\n",
    "                    if tokens[1] == \"true\":\n",
    "                        class_labels = True\n",
    "\n",
    "                    elif tokens[1] == \"false\":\n",
    "                        class_labels = False\n",
    "\n",
    "                    else:\n",
    "                        raise TsFileParseException(\"invalid classLabel value\")\n",
    "\n",
    "                    # Check if we have any associated class values\n",
    "\n",
    "                    if token_len == 2 and class_labels:\n",
    "                        raise TsFileParseException(\"if the classlabel tag is true then class values must be supplied\")\n",
    "\n",
    "                    has_class_labels_tag = True\n",
    "                    class_label_list = [token.strip() for token in tokens[2:]]\n",
    "                    metadata_started = True\n",
    "\n",
    "                # Check if this line contains the start of data\n",
    "\n",
    "                elif line.startswith(\"@data\"):\n",
    "\n",
    "                    if line != \"@data\":\n",
    "                        raise TsFileParseException(\"data tag should not have an associated value\")\n",
    "\n",
    "                    if data_started and not metadata_started:\n",
    "                        raise TsFileParseException(\"metadata must come before data\")\n",
    "\n",
    "                    else:\n",
    "                        has_data_tag = True\n",
    "                        data_started = True\n",
    "\n",
    "                # If the 'data tag has been found then metadata has been parsed and data can be loaded\n",
    "\n",
    "                elif data_started:\n",
    "\n",
    "                    # Check that a full set of metadata has been provided\n",
    "\n",
    "                    if (not has_problem_name_tag or not has_timestamps_tag or not has_univariate_tag \n",
    "                        or not has_class_labels_tag or not has_data_tag):\n",
    "                        raise TsFileParseException(\"a full set of metadata has not been provided before the data\")\n",
    "\n",
    "                    # Replace any missing values with the value specified\n",
    "\n",
    "                    line = line.replace(\"?\", replace_missing_vals_with)\n",
    "\n",
    "                    # Check if we dealing with data that has timestamps\n",
    "\n",
    "                    if timestamps:\n",
    "\n",
    "                        # We're dealing with timestamps so cannot just split line on ':' as timestamps may contain one\n",
    "\n",
    "                        has_another_value = False\n",
    "                        has_another_dimension = False\n",
    "\n",
    "                        timestamps_for_dimension = []\n",
    "                        values_for_dimension = []\n",
    "\n",
    "                        this_line_num_dimensions = 0\n",
    "                        line_len = len(line)\n",
    "                        char_num = 0\n",
    "\n",
    "                        while char_num < line_len:\n",
    "\n",
    "                            # Move through any spaces\n",
    "\n",
    "                            while char_num < line_len and str.isspace(line[char_num]):\n",
    "                                char_num += 1\n",
    "\n",
    "                            # See if there is any more data to read in or if we should validate that read thus far\n",
    "\n",
    "                            if char_num < line_len:\n",
    "\n",
    "                                # See if we have an empty dimension (i.e. no values)\n",
    "\n",
    "                                if line[char_num] == \":\":\n",
    "                                    if len(instance_list) < (this_line_num_dimensions + 1):\n",
    "                                        instance_list.append([])\n",
    "\n",
    "                                    instance_list[this_line_num_dimensions].append(pd.Series())\n",
    "                                    this_line_num_dimensions += 1\n",
    "\n",
    "                                    has_another_value = False\n",
    "                                    has_another_dimension = True\n",
    "\n",
    "                                    timestamps_for_dimension = []\n",
    "                                    values_for_dimension = []\n",
    "\n",
    "                                    char_num += 1\n",
    "\n",
    "                                else:\n",
    "\n",
    "                                    # Check if we have reached a class label\n",
    "\n",
    "                                    if line[char_num] != \"(\" and class_labels:\n",
    "\n",
    "                                        class_val = line[char_num:].strip()\n",
    "\n",
    "                                        if class_val not in class_label_list:\n",
    "                                            raise TsFileParseException(\"the class value '\" + class_val + \"' on line \" + \n",
    "                                            str(line_num + 1) + \" is not valid\")\n",
    "\n",
    "                                        class_val_list.append(class_val)\n",
    "                                        char_num = line_len\n",
    "\n",
    "                                        has_another_value = False\n",
    "                                        has_another_dimension = False\n",
    "\n",
    "                                        timestamps_for_dimension = []\n",
    "                                        values_for_dimension = []\n",
    "\n",
    "                                    else:\n",
    "\n",
    "                                        # Read in the data contained within the next tuple\n",
    "\n",
    "                                        if line[char_num] != \"(\" and not class_labels:\n",
    "                                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                                                \" on line \" + str(line_num + 1) + \" does not start with a '('\")\n",
    "\n",
    "                                        char_num += 1\n",
    "                                        tuple_data = \"\"\n",
    "\n",
    "                                        while char_num < line_len and line[char_num] != \")\":\n",
    "                                            tuple_data += line[char_num]\n",
    "                                            char_num += 1\n",
    "\n",
    "                                        if char_num >= line_len or line[char_num] != \")\":\n",
    "                                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) +\n",
    "                                            \" on line \" + str(line_num + 1) + \" does not end with a ')'\")\n",
    "\n",
    "                                        # Read in any spaces immediately after the current tuple\n",
    "\n",
    "                                        char_num += 1\n",
    "\n",
    "                                        while char_num < line_len and str.isspace(line[char_num]):\n",
    "                                            char_num += 1\n",
    "\n",
    "                                        # Check if there is another value or dimension to process after this tuple\n",
    "\n",
    "                                        if char_num >= line_len:\n",
    "                                            has_another_value = False\n",
    "                                            has_another_dimension = False\n",
    "\n",
    "                                        elif line[char_num] == \",\":\n",
    "                                            has_another_value = True\n",
    "                                            has_another_dimension = False\n",
    "\n",
    "                                        elif line[char_num] == \":\":\n",
    "                                            has_another_value = False\n",
    "                                            has_another_dimension = True\n",
    "\n",
    "                                        char_num += 1\n",
    "\n",
    "                                        # Get the numeric value for the tuple by reading from the end of the tuple data backwards                                               to the last comma\n",
    "\n",
    "                                        last_comma_index = tuple_data.rfind(',')\n",
    "\n",
    "                                        if last_comma_index == -1:\n",
    "                                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                                            \" on line \" + str(line_num + 1) + \" contains a tuple that has no comma inside of it\")\n",
    "\n",
    "                                        try:\n",
    "                                            value = tuple_data[last_comma_index + 1:]\n",
    "                                            value = float(value)\n",
    "\n",
    "                                        except ValueError:\n",
    "                                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                                            \" on line \" + str(line_num + 1) + \n",
    "                                            \" contains a tuple that does not have a valid numeric value\")\n",
    "\n",
    "                                        # Check the type of timestamp that we have\n",
    "\n",
    "                                        timestamp = tuple_data[0: last_comma_index]\n",
    "\n",
    "                                        try:\n",
    "                                            timestamp = int(timestamp)\n",
    "                                            timestamp_is_int = True\n",
    "                                            timestamp_is_timestamp = False\n",
    "\n",
    "                                        except ValueError:\n",
    "                                            timestamp_is_int = False\n",
    "\n",
    "                                        if not timestamp_is_int:\n",
    "                                            try:\n",
    "                                                timestamp = timestamp.strip()\n",
    "                                                timestamp_is_timestamp = True\n",
    "\n",
    "                                            except ValueError:\n",
    "                                                timestamp_is_timestamp = False\n",
    "\n",
    "                                        # Make sure that the timestamps in the file (not just this dimension or case) are consistent\n",
    "\n",
    "                                        if not timestamp_is_timestamp and not timestamp_is_int:\n",
    "                                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                                            \" on line \" + str(line_num + 1) + \" contains a tuple that has an invalid timestamp '\" + \n",
    "                                            timestamp + \"'\")\n",
    "\n",
    "                                        if previous_timestamp_was_int is not None and previous_timestamp_was_int and \\\n",
    "                                         not timestamp_is_int:\n",
    "                                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                                            \" on line \" + str(line_num + 1) + \n",
    "                                            \" contains tuples where the timestamp format is inconsistent\")\n",
    "\n",
    "                                        if previous_timestamp_was_timestamp is not None and previous_timestamp_was_timestamp and \\\n",
    "                                        not timestamp_is_timestamp:\n",
    "                                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) +\n",
    "                                            \" on line \" + str(line_num + 1) + \n",
    "                                            \" contains tuples where the timestamp format is inconsistent\")\n",
    "\n",
    "                                        # Store the values\n",
    "\n",
    "                                        timestamps_for_dimension += [timestamp]\n",
    "                                        values_for_dimension += [value]\n",
    "\n",
    "                                        #  If this was our first tuple then we store the type of timestamp we had\n",
    "\n",
    "                                        if previous_timestamp_was_timestamp is None and timestamp_is_timestamp:\n",
    "                                            previous_timestamp_was_timestamp = True\n",
    "                                            previous_timestamp_was_int = False\n",
    "\n",
    "                                        if previous_timestamp_was_int is None and timestamp_is_int:\n",
    "                                            previous_timestamp_was_timestamp = False\n",
    "                                            previous_timestamp_was_int = True\n",
    "\n",
    "                                        # See if we should add the data for this dimension\n",
    "\n",
    "                                        if not has_another_value:\n",
    "                                            if len(instance_list) < (this_line_num_dimensions + 1):\n",
    "                                                instance_list.append([])\n",
    "\n",
    "                                            if timestamp_is_timestamp:\n",
    "                                                timestamps_for_dimension = pd.DatetimeIndex(timestamps_for_dimension)\n",
    "\n",
    "                                            instance_list[this_line_num_dimensions].append(pd.Series(index=timestamps_for_dimension\n",
    "                                            , data=values_for_dimension))\n",
    "                                            this_line_num_dimensions += 1\n",
    "\n",
    "                                            timestamps_for_dimension = []\n",
    "                                            values_for_dimension = []\n",
    "\n",
    "                            elif has_another_value:\n",
    "                                raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                                \" on line \" + str(line_num + 1) + \" ends with a ',' that is not followed by another tuple\")\n",
    "\n",
    "                            elif has_another_dimension and class_labels:\n",
    "                                raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                                \" on line \" + str(line_num + 1) + \" ends with a ':' while it should list a class value\")\n",
    "\n",
    "                            elif has_another_dimension and not class_labels:\n",
    "                                if len(instance_list) < (this_line_num_dimensions + 1):\n",
    "                                    instance_list.append([])\n",
    "\n",
    "                                instance_list[this_line_num_dimensions].append(pd.Series(dtype=np.float32))\n",
    "                                this_line_num_dimensions += 1\n",
    "                                num_dimensions = this_line_num_dimensions\n",
    "\n",
    "                            # If this is the 1st line of data we have seen then note the dimensions\n",
    "\n",
    "                            if not has_another_value and not has_another_dimension:\n",
    "                                if num_dimensions is None:\n",
    "                                    num_dimensions = this_line_num_dimensions\n",
    "\n",
    "                                if num_dimensions != this_line_num_dimensions:\n",
    "                                    raise TsFileParseException(\"line \" + str(line_num + 1) + \n",
    "                                    \" does not have the same number of dimensions as the previous line of data\")\n",
    "\n",
    "                        # Check that we are not expecting some more data, and if not, store that processed above\n",
    "\n",
    "                        if has_another_value:\n",
    "                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                            \" on line \" + str(line_num + 1) + \" ends with a ',' that is not followed by another tuple\")\n",
    "\n",
    "                        elif has_another_dimension and class_labels:\n",
    "                            raise TsFileParseException(\"dimension \" + str(this_line_num_dimensions + 1) + \n",
    "                            \" on line \" + str(line_num + 1) + \" ends with a ':' while it should list a class value\")\n",
    "\n",
    "                        elif has_another_dimension and not class_labels:\n",
    "                            if len(instance_list) < (this_line_num_dimensions + 1):\n",
    "                                instance_list.append([])\n",
    "\n",
    "                            instance_list[this_line_num_dimensions].append(pd.Series())\n",
    "                            this_line_num_dimensions += 1\n",
    "                            num_dimensions = this_line_num_dimensions\n",
    "\n",
    "                        # If this is the 1st line of data we have seen then note the dimensions\n",
    "\n",
    "                        if not has_another_value and num_dimensions != this_line_num_dimensions:\n",
    "                            raise TsFileParseException(\"line \" + str(line_num + 1) + \n",
    "                            \" does not have the same number of dimensions as the previous line of data\")\n",
    "\n",
    "                        # Check if we should have class values, and if so that they are contained in those listed in the metadata\n",
    "\n",
    "                        if class_labels and len(class_val_list) == 0:\n",
    "                            raise TsFileParseException(\"the cases have no associated class values\")\n",
    "\n",
    "                    else:\n",
    "                        dimensions = line.split(\":\")\n",
    "\n",
    "                        # If first row then note the number of dimensions (that must be the same for all cases)\n",
    "\n",
    "                        if is_first_case:\n",
    "                            num_dimensions = len(dimensions)\n",
    "\n",
    "                            if class_labels:\n",
    "                                num_dimensions -= 1\n",
    "\n",
    "                            for dim in range(0, num_dimensions):\n",
    "                                instance_list.append([])\n",
    "\n",
    "                            is_first_case = False\n",
    "\n",
    "                        # See how many dimensions that the case whose data in represented in this line has\n",
    "\n",
    "                        this_line_num_dimensions = len(dimensions)\n",
    "\n",
    "                        if class_labels:\n",
    "                            this_line_num_dimensions -= 1\n",
    "\n",
    "                        # All dimensions should be included for all series, even if they are empty\n",
    "\n",
    "                        if this_line_num_dimensions != num_dimensions:\n",
    "                            raise TsFileParseException(\"inconsistent number of dimensions\")\n",
    "\n",
    "                        # Process the data for each dimension\n",
    "\n",
    "                        for dim in range(0, num_dimensions):\n",
    "                            dimension = dimensions[dim].strip()\n",
    "\n",
    "                            if dimension:\n",
    "#                                 data_series = dimension.split(\",\")\n",
    "#                                 data_series = [float(i) for i in data_series]\n",
    "#                                 instance_list[dim].append(pd.Series(data_series))\n",
    "                                \n",
    "#                                 instance_list[dim].append(np.array(dimensions[dim].strip().split(','), dtype=np.float32))\n",
    "                                instance_list[dim].append(np.array(dimensions[dim].split(','), dtype=np.float32))\n",
    "#                                 instance_list[dim].append(np.fromiter(dimensions[dim].strip().split(','), dtype=np.float32))\n",
    "\n",
    "                            else:\n",
    "#                                 instance_list[dim].append(pd.Series())\n",
    "                                instance_list[dim].append([])\n",
    "\n",
    "                        if class_labels:\n",
    "                            class_val_list.append(dimensions[num_dimensions].strip())\n",
    "\n",
    "            line_num += 1\n",
    "\n",
    "    # Check that the file was not empty\n",
    "\n",
    "    if line_num:\n",
    "        # Check that the file contained both metadata and data\n",
    "\n",
    "        if metadata_started and not (has_problem_name_tag and has_timestamps_tag and has_univariate_tag and \n",
    "        has_class_labels_tag and has_data_tag):\n",
    "            raise TsFileParseException(\"metadata incomplete\")\n",
    "\n",
    "        elif metadata_started and not data_started:\n",
    "            raise TsFileParseException(\"file contained metadata but no data\")\n",
    "\n",
    "        elif metadata_started and data_started and len(instance_list) == 0:\n",
    "            raise TsFileParseException(\"file contained metadata but no data\")\n",
    "\n",
    "#         # Create a DataFrame from the data parsed above\n",
    "\n",
    "#         data = pd.DataFrame(dtype=np.float32)\n",
    "\n",
    "#         for dim in range(0, num_dimensions):\n",
    "#             data['dim_' + str(dim)] = instance_list[dim]\n",
    "\n",
    "#         # Check if we should return any associated class labels separately\n",
    "\n",
    "#         if class_labels:\n",
    "#             if return_separate_X_and_y:\n",
    "#                 return data, np.asarray(class_val_list)\n",
    "\n",
    "#             else:\n",
    "#                 data['class_vals'] = pd.Series(class_val_list)\n",
    "#                 return data\n",
    "#         else:\n",
    "#             return data\n",
    "\n",
    "        \n",
    "        # Create a numpy array\n",
    "            \n",
    "        #instance_list has a shape of (dimensions, nb_samples, seq_lenght)\n",
    "        #for the NATOPS_Train.arff it would be (24, 180, 51)\n",
    "        #convert python list to numpy array and traspose the 2 first dimensions -> (180, 24, 51)\n",
    "        data_array = np.asarray(instance_list).transpose(1,0,2) \n",
    "        y = np.asarray(class_val_list)\n",
    "\n",
    "        return data_array, y\n",
    "    \n",
    "\n",
    "    else:\n",
    "        raise TsFileParseException(\"empty file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Path('/home/farid/.fastai/data/NATOPS/NATOPS_TRAIN.ts')"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname_train = path_data/f'{dsname}/{dsname}_TRAIN.ts'\n",
    "fname_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((180, 24, 51), (180,))"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_ts, train_y_ts = load_from_tsfile_to_array(fname_train)\n",
    "train_x_ts.shape, train_y_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(24, 51)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_ts[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.445743"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_ts[10][0][30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_UCR_univariate_list():\n",
    "    return [\n",
    "        'ACSF1', 'Adiac', 'AllGestureWiimoteX', 'AllGestureWiimoteY',\n",
    "        'AllGestureWiimoteZ', 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken',\n",
    "        'BME', 'Car', 'CBF', 'Chinatown', 'ChlorineConcentration',\n",
    "        'CinCECGtorso', 'Coffee', 'Computers', 'CricketX', 'CricketY',\n",
    "        'CricketZ', 'Crop', 'DiatomSizeReduction',\n",
    "        'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect',\n",
    "        'DistalPhalanxTW', 'DodgerLoopDay', 'DodgerLoopGame',\n",
    "        'DodgerLoopWeekend', 'Earthquakes', 'ECG200', 'ECG5000', 'ECGFiveDays',\n",
    "        'ElectricDevices', 'EOGHorizontalSignal', 'EOGVerticalSignal',\n",
    "        'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR', 'FiftyWords',\n",
    "        'Fish', 'FordA', 'FordB', 'FreezerRegularTrain', 'FreezerSmallTrain',\n",
    "        'Fungi', 'GestureMidAirD1', 'GestureMidAirD2', 'GestureMidAirD3',\n",
    "        'GesturePebbleZ1', 'GesturePebbleZ2', 'GunPoint', 'GunPointAgeSpan',\n",
    "        'GunPointMaleVersusFemale', 'GunPointOldVersusYoung', 'Ham',\n",
    "        'HandOutlines', 'Haptics', 'Herring', 'HouseTwenty', 'InlineSkate',\n",
    "        'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound',\n",
    "        'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2',\n",
    "        'Lightning7', 'Mallat', 'Meat', 'MedicalImages', 'MelbournePedestrian',\n",
    "        'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect',\n",
    "        'MiddlePhalanxTW', 'MixedShapes', 'MixedShapesSmallTrain',\n",
    "        'MoteStrain', 'NonInvasiveFetalECGThorax1',\n",
    "        'NonInvasiveFetalECGThorax2', 'OliveOil', 'OSULeaf',\n",
    "        'PhalangesOutlinesCorrect', 'Phoneme', 'PickupGestureWiimoteZ',\n",
    "        'PigAirwayPressure', 'PigArtPressure', 'PigCVP', 'PLAID', 'Plane',\n",
    "        'PowerCons', 'ProximalPhalanxOutlineAgeGroup',\n",
    "        'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW',\n",
    "        'RefrigerationDevices', 'Rock', 'ScreenType', 'SemgHandGenderCh2',\n",
    "        'SemgHandMovementCh2', 'SemgHandSubjectCh2', 'ShakeGestureWiimoteZ',\n",
    "        'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SmoothSubspace',\n",
    "        'SonyAIBORobotSurface1', 'SonyAIBORobotSurface2', 'StarlightCurves',\n",
    "        'Strawberry', 'SwedishLeaf', 'Symbols', 'SyntheticControl',\n",
    "        'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG',\n",
    "        'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll', 'UWaveGestureLibraryX',\n",
    "        'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'Wafer', 'Wine',\n",
    "        'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga'\n",
    "    ]\n",
    "\n",
    "def get_UCR_multivariate_list():\n",
    "    return [\n",
    "        'ArticularyWordRecognition', 'AtrialFibrillation', 'BasicMotions',\n",
    "        'CharacterTrajectories', 'Cricket', 'DuckDuckGeese', 'EigenWorms',\n",
    "        'Epilepsy', 'EthanolConcentration', 'ERing', 'FaceDetection',\n",
    "        'FingerMovements', 'HandMovementDirection', 'Handwriting', 'Heartbeat',\n",
    "        'JapaneseVowels', 'Libras', 'LSST', 'InsectWingbeat', 'MotorImagery',\n",
    "        'NATOPS', 'PenDigits', 'PEMS-SF', 'PhonemeSpectra', 'RacketSports',\n",
    "        'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits',\n",
    "        'StandWalkJump', 'UWaveGestureLibrary'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "def camel2snake(name):\n",
    "    \"Convert CamelCase to snake_case\"\n",
    "    s1   = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
    "\n",
    "def camel2capitalsnake(name):\n",
    "    return camel2snake(name).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = 'ArticularyWordRecognition'\n",
    "# s2 = camel2snake(s).upper()\n",
    "# s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls_ts = [f'{camel2capitalsnake(n)} = {n}.zip'     for n in get_UCR_multivariate_list()]\n",
    "# urls_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #hide   \n",
    "    # MULTI_ARTICULARY_WORD_RECOGNITION = f'{URL}ArticularyWordRecognition.zip',\n",
    "    # MULTI_ATRIAL_FIBRILLATION = f'{URL}AtrialFibrillation.zip',\n",
    "    # MULTI_BASIC_MOTIONS = f'{URL}BasicMotions.zip',\n",
    "    # MULTI_CHARACTER_TRAJECTORIES = f'{URL}CharacterTrajectories.zip',\n",
    "    # MULTI_CRICKET = f'{URL}Cricket.zip',\n",
    "    # MULTI_DUCK_DUCK_GEESE = f'{URL}DuckDuckGeese.zip',\n",
    "    # MULTI_EIGEN_WORMS = f'{URL}EigenWorms.zip',\n",
    "    # MULTI_EPILEPSY = f'{URL}Epilepsy.zip',\n",
    "    # MULTI_ETHANOL_CONCENTRATION = f'{URL}EthanolConcentration.zip',\n",
    "    # MULTI_ERING = f'{URL}ERing.zip',\n",
    "    # MULTI_FACE_DETECTION = f'{URL}FaceDetection.zip',\n",
    "    # MULTI_FINGER_MOVEMENTS = f'{URL}FingerMovements.zip',\n",
    "    # MULTI_HAND_MOVEMENT_DIRECTION = f'{URL}HandMovementDirection.zip',\n",
    "    # MULTI_HANDWRITING = f'{URL}Handwriting.zip',\n",
    "    # MULTI_HEARTBEAT = f'{URL}Heartbeat.zip',\n",
    "    # MULTI_JAPANESE_VOWELS = f'{URL}JapaneseVowels.zip',\n",
    "    # MULTI_LIBRAS = f'{URL}Libras.zip',\n",
    "    # MULTI_LSST = f'{URL}LSST.zip',\n",
    "    # MULTI_INSECT_WINGBEAT = f'{URL}InsectWingbeat.zip',\n",
    "    # MULTI_MOTOR_IMAGERY = f'{URL}MotorImagery.zip',\n",
    "    # MULTI_NATOPS = f'{URL}NATOPS.zip',\n",
    "    # MULTI_PEN_DIGITS = f'{URL}PenDigits.zip',\n",
    "    # MULTI_PEMS-SF = f'{URL}PEMS-SF.zip',\n",
    "    # MULTI_PHONEME_SPECTRA = f'{URL}PhonemeSpectra.zip',\n",
    "    # MULTI_RACKET_SPORTS = f'{URL}RacketSports.zip',\n",
    "    # MULTI_SELF_REGULATION_SCP1 = f'{URL}SelfRegulationSCP1.zip',\n",
    "    # MULTI_SELF_REGULATION_SCP2 = f'{URL}SelfRegulationSCP2.zip',\n",
    "    # MULTI_SPOKEN_ARABIC_DIGITS = f'{URL}SpokenArabicDigits.zip',\n",
    "    # MULTI_STAND_WALK_JUMP = f'{URL}StandWalkJump.zip',\n",
    "    # MULTI_U_WAVE_GESTURE_LIBRARY = f'{URL}UWaveGestureLibrary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls_ts = [f'{camel2capitalsnake(n)} = {n}.zip'     for n in get_UCR_univariate_list()]\n",
    "# urls_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "    # UNI_ACSF1 = f'{URL}ACSF1.zip,\n",
    "    # UNI_ADIAC = f'{URL}Adiac.zip,\n",
    "    # UNI_ALL_GESTURE_WIIMOTE_X = f'{URL}AllGestureWiimoteX.zip,\n",
    "    # UNI_ALL_GESTURE_WIIMOTE_Y = f'{URL}AllGestureWiimoteY.zip,\n",
    "    # UNI_ALL_GESTURE_WIIMOTE_Z = f'{URL}AllGestureWiimoteZ.zip,\n",
    "    # UNI_ARROW_HEAD = f'{URL}ArrowHead.zip,\n",
    "    # UNI_BEEF = f'{URL}Beef.zip,\n",
    "    # UNI_BEETLE_FLY = f'{URL}BeetleFly.zip,\n",
    "    # UNI_BIRD_CHICKEN = f'{URL}BirdChicken.zip,\n",
    "    # UNI_BME = f'{URL}BME.zip,\n",
    "    # UNI_CAR = f'{URL}Car.zip,\n",
    "    # UNI_CBF = f'{URL}CBF.zip,\n",
    "    # UNI_CHINATOWN = f'{URL}Chinatown.zip,\n",
    "    # UNI_CHLORINE_CONCENTRATION = f'{URL}ChlorineConcentration.zip,\n",
    "    # UNI_CIN_CEC_GTORSO = f'{URL}CinCECGtorso.zip,\n",
    "    # UNI_COFFEE = f'{URL}Coffee.zip,\n",
    "    # UNI_COMPUTERS = f'{URL}Computers.zip,\n",
    "    # UNI_CRICKET_X = f'{URL}CricketX.zip,\n",
    "    # UNI_CRICKET_Y = f'{URL}CricketY.zip,\n",
    "    # UNI_CRICKET_Z = f'{URL}CricketZ.zip,\n",
    "    # UNI_CROP = f'{URL}Crop.zip,\n",
    "    # UNI_DIATOM_SIZE_REDUCTION = f'{URL}DiatomSizeReduction.zip,\n",
    "    # UNI_DISTAL_PHALANX_OUTLINE_AGE_GROUP = f'{URL}DistalPhalanxOutlineAgeGroup.zip,\n",
    "    # UNI_DISTAL_PHALANX_OUTLINE_CORRECT = f'{URL}DistalPhalanxOutlineCorrect.zip,\n",
    "    # UNI_DISTAL_PHALANX_TW = f'{URL}DistalPhalanxTW.zip,\n",
    "    # UNI_DODGER_LOOP_DAY = f'{URL}DodgerLoopDay.zip,\n",
    "    # UNI_DODGER_LOOP_GAME = f'{URL}DodgerLoopGame.zip,\n",
    "    # UNI_DODGER_LOOP_WEEKEND = f'{URL}DodgerLoopWeekend.zip,\n",
    "    # UNI_EARTHQUAKES = f'{URL}Earthquakes.zip,\n",
    "    # UNI_ECG200 = f'{URL}ECG200.zip,\n",
    "    # UNI_ECG5000 = f'{URL}ECG5000.zip,\n",
    "    # UNI_ECG_FIVE_DAYS = f'{URL}ECGFiveDays.zip,\n",
    "    # UNI_ELECTRIC_DEVICES = f'{URL}ElectricDevices.zip,\n",
    "    # UNI_EOG_HORIZONTAL_SIGNAL = f'{URL}EOGHorizontalSignal.zip,\n",
    "    # UNI_EOG_VERTICAL_SIGNAL = f'{URL}EOGVerticalSignal.zip,\n",
    "    # UNI_ETHANOL_LEVEL = f'{URL}EthanolLevel.zip,\n",
    "    # UNI_FACE_ALL = f'{URL}FaceAll.zip,\n",
    "    # UNI_FACE_FOUR = f'{URL}FaceFour.zip,\n",
    "    # UNI_FACES_UCR = f'{URL}FacesUCR.zip,\n",
    "    # UNI_FIFTY_WORDS = f'{URL}FiftyWords.zip,\n",
    "    # UNI_FISH = f'{URL}Fish.zip,\n",
    "    # UNI_FORD_A = f'{URL}FordA.zip,\n",
    "    # UNI_FORD_B = f'{URL}FordB.zip,\n",
    "    # UNI_FREEZER_REGULAR_TRAIN = f'{URL}FreezerRegularTrain.zip,\n",
    "    # UNI_FREEZER_SMALL_TRAIN = f'{URL}FreezerSmallTrain.zip,\n",
    "    # UNI_FUNGI = f'{URL}Fungi.zip,\n",
    "    # UNI_GESTURE_MID_AIR_D1 = f'{URL}GestureMidAirD1.zip,\n",
    "    # UNI_GESTURE_MID_AIR_D2 = f'{URL}GestureMidAirD2.zip,\n",
    "    # UNI_GESTURE_MID_AIR_D3 = f'{URL}GestureMidAirD3.zip,\n",
    "    # UNI_GESTURE_PEBBLE_Z1 = f'{URL}GesturePebbleZ1.zip,\n",
    "    # UNI_GESTURE_PEBBLE_Z2 = f'{URL}GesturePebbleZ2.zip,\n",
    "    # UNI_GUN_POINT = f'{URL}GunPoint.zip,\n",
    "    # UNI_GUN_POINT_AGE_SPAN = f'{URL}GunPointAgeSpan.zip,\n",
    "    # UNI_GUN_POINT_MALE_VERSUS_FEMALE = f'{URL}GunPointMaleVersusFemale.zip,\n",
    "    # UNI_GUN_POINT_OLD_VERSUS_YOUNG = f'{URL}GunPointOldVersusYoung.zip,\n",
    "    # UNI_HAM = f'{URL}Ham.zip,\n",
    "    # UNI_HAND_OUTLINES = f'{URL}HandOutlines.zip,\n",
    "    # UNI_HAPTICS = f'{URL}Haptics.zip,\n",
    "    # UNI_HERRING = f'{URL}Herring.zip,\n",
    "    # UNI_HOUSE_TWENTY = f'{URL}HouseTwenty.zip,\n",
    "    # UNI_INLINE_SKATE = f'{URL}InlineSkate.zip,\n",
    "    # UNI_INSECT_EPG_REGULAR_TRAIN = f'{URL}InsectEPGRegularTrain.zip,\n",
    "    # UNI_INSECT_EPG_SMALL_TRAIN = f'{URL}InsectEPGSmallTrain.zip,\n",
    "    # UNI_INSECT_WINGBEAT_SOUND = f'{URL}InsectWingbeatSound.zip,\n",
    "    # UNI_ITALY_POWER_DEMAND = f'{URL}ItalyPowerDemand.zip,\n",
    "    # UNI_LARGE_KITCHEN_APPLIANCES = f'{URL}LargeKitchenAppliances.zip,\n",
    "    # UNI_LIGHTNING2 = f'{URL}Lightning2.zip,\n",
    "    # UNI_LIGHTNING7 = f'{URL}Lightning7.zip,\n",
    "    # UNI_MALLAT = f'{URL}Mallat.zip,\n",
    "    # UNI_MEAT = f'{URL}Meat.zip,\n",
    "    # UNI_MEDICAL_IMAGES = f'{URL}MedicalImages.zip,\n",
    "    # UNI_MELBOURNE_PEDESTRIAN = f'{URL}MelbournePedestrian.zip,\n",
    "    # UNI_MIDDLE_PHALANX_OUTLINE_AGE_GROUP = f'{URL}MiddlePhalanxOutlineAgeGroup.zip,\n",
    "    # UNI_MIDDLE_PHALANX_OUTLINE_CORRECT = f'{URL}MiddlePhalanxOutlineCorrect.zip,\n",
    "    # UNI_MIDDLE_PHALANX_TW = f'{URL}MiddlePhalanxTW.zip,\n",
    "    # UNI_MIXED_SHAPES = f'{URL}MixedShapes.zip,\n",
    "    # UNI_MIXED_SHAPES_SMALL_TRAIN = f'{URL}MixedShapesSmallTrain.zip,\n",
    "    # UNI_MOTE_STRAIN = f'{URL}MoteStrain.zip,\n",
    "    # UNI_NON_INVASIVE_FETAL_ECG_THORAX1 = f'{URL}NonInvasiveFetalECGThorax1.zip,\n",
    "    # UNI_NON_INVASIVE_FETAL_ECG_THORAX2 = f'{URL}NonInvasiveFetalECGThorax2.zip,\n",
    "    # UNI_OLIVE_OIL = f'{URL}OliveOil.zip,\n",
    "    # UNI_OSU_LEAF = f'{URL}OSULeaf.zip,\n",
    "    # UNI_PHALANGES_OUTLINES_CORRECT = f'{URL}PhalangesOutlinesCorrect.zip,\n",
    "    # UNI_PHONEME = f'{URL}Phoneme.zip,\n",
    "    # UNI_PICKUP_GESTURE_WIIMOTE_Z = f'{URL}PickupGestureWiimoteZ.zip,\n",
    "    # UNI_PIG_AIRWAY_PRESSURE = f'{URL}PigAirwayPressure.zip,\n",
    "    # UNI_PIG_ART_PRESSURE = f'{URL}PigArtPressure.zip,\n",
    "    # UNI_PIG_CVP = f'{URL}PigCVP.zip,\n",
    "    # UNI_PLAID = f'{URL}PLAID.zip,\n",
    "    # UNI_PLANE = f'{URL}Plane.zip,\n",
    "    # UNI_POWER_CONS = f'{URL}PowerCons.zip,\n",
    "    # UNI_PROXIMAL_PHALANX_OUTLINE_AGE_GROUP = f'{URL}ProximalPhalanxOutlineAgeGroup.zip,\n",
    "    # UNI_PROXIMAL_PHALANX_OUTLINE_CORRECT = f'{URL}ProximalPhalanxOutlineCorrect.zip,\n",
    "    # UNI_PROXIMAL_PHALANX_TW = f'{URL}ProximalPhalanxTW.zip,\n",
    "    # UNI_REFRIGERATION_DEVICES = f'{URL}RefrigerationDevices.zip,\n",
    "    # UNI_ROCK = f'{URL}Rock.zip,\n",
    "    # UNI_SCREEN_TYPE = f'{URL}ScreenType.zip,\n",
    "    # UNI_SEMG_HAND_GENDER_CH2 = f'{URL}SemgHandGenderCh2.zip,\n",
    "    # UNI_SEMG_HAND_MOVEMENT_CH2 = f'{URL}SemgHandMovementCh2.zip,\n",
    "    # UNI_SEMG_HAND_SUBJECT_CH2 = f'{URL}SemgHandSubjectCh2.zip,\n",
    "    # UNI_SHAKE_GESTURE_WIIMOTE_Z = f'{URL}ShakeGestureWiimoteZ.zip,\n",
    "    # UNI_SHAPELET_SIM = f'{URL}ShapeletSim.zip,\n",
    "    # UNI_SHAPES_ALL = f'{URL}ShapesAll.zip,\n",
    "    # UNI_SMALL_KITCHEN_APPLIANCES = f'{URL}SmallKitchenAppliances.zip,\n",
    "    # UNI_SMOOTH_SUBSPACE = f'{URL}SmoothSubspace.zip,\n",
    "    # UNI_SONY_AIBO_ROBOT_SURFACE1 = f'{URL}SonyAIBORobotSurface1.zip,\n",
    "    # UNI_SONY_AIBO_ROBOT_SURFACE2 = f'{URL}SonyAIBORobotSurface2.zip,\n",
    "    # UNI_STARLIGHT_CURVES = f'{URL}StarlightCurves.zip,\n",
    "    # UNI_STRAWBERRY = f'{URL}Strawberry.zip,\n",
    "    # UNI_SWEDISH_LEAF = f'{URL}SwedishLeaf.zip,\n",
    "    # UNI_SYMBOLS = f'{URL}Symbols.zip,\n",
    "    # UNI_SYNTHETIC_CONTROL = f'{URL}SyntheticControl.zip,\n",
    "    # UNI_TOE_SEGMENTATION1 = f'{URL}ToeSegmentation1.zip,\n",
    "    # UNI_TOE_SEGMENTATION2 = f'{URL}ToeSegmentation2.zip,\n",
    "    # UNI_TRACE = f'{URL}Trace.zip,\n",
    "    # UNI_TWO_LEAD_ECG = f'{URL}TwoLeadECG.zip,\n",
    "    # UNI_TWO_PATTERNS = f'{URL}TwoPatterns.zip,\n",
    "    # UNI_UMD = f'{URL}UMD.zip,\n",
    "    # UNI_U_WAVE_GESTURE_LIBRARY_ALL = f'{URL}UWaveGestureLibraryAll.zip,\n",
    "    # UNI_U_WAVE_GESTURE_LIBRARY_X = f'{URL}UWaveGestureLibraryX.zip,\n",
    "    # UNI_U_WAVE_GESTURE_LIBRARY_Y = f'{URL}UWaveGestureLibraryY.zip,\n",
    "    # UNI_U_WAVE_GESTURE_LIBRARY_Z = f'{URL}UWaveGestureLibraryZ.zip,\n",
    "    # UNI_WAFER = f'{URL}Wafer.zip,\n",
    "    # UNI_WINE = f'{URL}Wine.zip,\n",
    "    # UNI_WORD_SYNONYMS = f'{URL}WordSynonyms.zip,\n",
    "    # UNI_WORMS = f'{URL}Worms.zip,\n",
    "    # UNI_WORMS_TWO_CLASS = f'{URL}WormsTwoClass.zip,\n",
    "    # UNI_YOGA = f'{URL}Yoga.zipUNI_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Converted 80_timeseries_data.ipynb.\nConverted 81_timeseries_core.ipynb.\nConverted Colab_timeseries_Tutorial copy.ipynb.\nConverted Colab_timeseries_Tutorial.ipynb.\nConverted index.ipynb.\nConverted univariate_timeseries_Tutorial.ipynb.\n"
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "# notebook2script(fname='80_timeseries_data.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "converting: 80_timeseries_data.ipynb\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export2html import _notebook2html\n",
    "# notebook2script()\n",
    "_notebook2html(fname='80_timeseries_data.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/ai-fast-track/timeseries/blob/master/images/blue-sea.jpg?raw=1\" width=\"1440\" height=\"840\" alt=\"\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
