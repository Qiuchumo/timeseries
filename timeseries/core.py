# AUTOGENERATED! DO NOT EDIT! File to edit: 81_timeseries_core.ipynb (unless otherwise specified).

__all__ = ['TensorTS', 'ToTensorTSBlock', 'ToTensorTS', 'LabelTS', 'get_stats', 'Normalize', 'SelfNormalize', 'TSBlock',
           'default_show_batch', 'get_grid', 'show_results']

# Cell
from fastai2.torch_basics import *
# from fastai2.test import *
from fastai2.data.all import *

# Cell
from .data import *

# Cell
class TensorTS(TensorBase):
    "Transform a 2D array into a Tensor"
#     def __rep__(): return f"{self.__class__.__name__}\n {self.shape}"
    def show(self, ctx=None, title=None, chs=None, leg=True, **kwargs):
        "Display timeseries plots for all selected channels list `chs`"
        if ctx is None: fig, ctx = plt.subplots()
        t = range(self.shape[1])
        chs_max = max(chs) if chs else 0
        channels = chs if (chs and (chs_max < self.shape[0])) else range(self.shape[0])
        for ch in channels:
            ctx.plot(t, self[ch], label='ch'+str(ch))
        if leg: ctx.legend(loc='upper right', ncol=2, framealpha=0.5)
        if title: ctx.set_title(title)

# Cell
class ToTensorTSBlock(Transform):
    "x : 2D numpy array"
    def encodes(self, x):
        return TensorTS(x)

# Cell
class ToTensorTS(Transform):
    "x : tuple representing (2D numpy array, Label)"
    def encodes(self, x):
        return TensorTS(x[0])

# Cell
class LabelTS(Transform):
    "x : tuple representing (2D numpy array, Label)"
    def encodes(self, x):
        return x[1]

# Cell
def get_stats(train, scale_type='standardize', scale_subtype='all_samples'):
    if scale_type == 'normalize':
        if scale_subtype == 'all_samples':
            train_min = train.min(keepdims=True)
            train_max = train.max(keepdims=True)
        elif scale_subtype == 'per_sample':
            train_min = train.min(axis=(1, 2), keepdims=True)
            train_max = train.max(axis=(1, 2), keepdims=True)
        elif scale_subtype == 'per_channel':
            train_min = train.min(axis=(0, 2), keepdims=True)
            train_max = train.max(axis=(0, 2), keepdims=True)
        elif scale_subtype == 'per_sample_per_channel':
            train_min = train.min(axis=(2), keepdims=True)
            train_max = train.max(axis=(2), keepdims=True)
        else:
            print('***** Please, select a valid  scaling_subtype *****')
            return
        min, max = train_min, train_max
        return min, max

    elif scale_type == 'standardize':
        print(scale_type)
        if scale_subtype == 'all_samples':
            train_mean = train.mean(keepdims=True)
            train_std = train.std(keepdims=True)
        elif scale_subtype == 'per_sample':
            train_mean = train.mean(axis=(1, 2), keepdims=True)
            train_std = train.std(axis=(1, 2), keepdims=True)
        elif scale_subtype == 'per_channel':
            train_mean = train.mean(axis=(0, 2), keepdims=True)
            train_std = train.std(axis=(0, 2), keepdims=True)
        elif scale_subtype == 'per_sample_per_channel':
            train_mean = train.mean(axis=(2), keepdims=True)
            train_std = train.std(axis=(2), keepdims=True)
        else:
            print('***** Please, select a valid  scaling_subtype *****')
            return
        mean, std = torch.tensor(train_mean), torch.tensor(train_std)
        return mean, std

    else:
        print('***** Please, select a valid  scaling_type *****')
        return

# Cell
@docs
class Normalize(Transform):
    "Normalize/denorm batch of `TimeseriesTensor`"
    order=99
    def __init__(self, mean, std, cuda=True):
        f = to_device if cuda else noop
        self.mean,self.std = f(mean), f(std)

    def encodes(self, x:TensorTS):
        print('Normalize - encodes')
        return (x-self.mean) / self.std
    def decodes(self, x:TensorTS):
        f = to_cpu if x.device.type=='cpu' else noop
        return (x*f(self.std) + f(self.mean))

    _docs=dict(encodes="Normalize batch", decodes="Denormalize batch")

# Cell
@docs
class SelfNormalize(Transform):
    "Normalize/denorm a `TimeseriesTensor` either per sample or per channel "
    order=99
    def __init__(self,  scale_type='standardize', scale_subtype='per_sample', scale_range=(-1, 1)):
        self.scale_type = scale_type
        self.scale_subtype = scale_subtype
        self.scale_range = scale_range


    def encodes(self, x:TensorTS):
#         print('normalize')
        if self.scale_type is None:
            return x

        if self.scale_type == 'normalize':
#             print('normalize')
            if self.scale_subtype == 'per_sample':
                self.min = (torch.min(x)).expand_as(x)
                self.max = (torch.max(x)).expand_as(x)
#                 print(self.min)
#                 print(self.max)
            elif self.scale_subtype == 'per_sample_per_channel':
#                 print('per_sample_per_channel')
                self.min = (torch.min(t, dim=1, keepdims=True).values).expand_as(x)
                self.max = (torch.max(t, dim=1, keepdims=True).values).expand_as(x)
#                 print(self.min)
#                 print(self.max)
            else: return x
            return ((x-self.min) / (self.max - self.min)) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]
        elif self.scale_type == 'standardize':
#             print('standardize')
            if self.scale_subtype == 'per_sample':
                self.mean = x.mean(axis=(0,1), keepdims=True)
                self.std = x.std(axis=(0,1), keepdims=True)
            elif self.scale_subtype == 'per_sample_per_channel':
                print('per_sample_per_channel')
#                 self.mean = x.mean(axis=(1), keepdims=True)
                self.std = x.std(axis=(1), keepdims=True)
            else: return x
            return (x-self.mean) / self.std
        else: return x
#     def decodes(self, x:TensorTS):
#         f = to_cpu if x.device.type=='cpu' else noop
#         return (x*f(self.std) + f(self.mean))

    _docs=dict(encodes="Normalize either per sample or per channel") #, decodes="Denormalize either per sample or per channel"

# Cell
def TSBlock():
    "`TransformBlock` for timeseries : Transform np array to TensorTS type"
    return TransformBlock(type_tfms=ToTensorTSBlock())

# Cell
def default_show_batch(x, y, its, ctxs=None, max_n=9, **kwargs):
    if ctxs is None: ctxs = Inf.nones
    ctxs = [b[0].show(ctx=c, title=b[1], **kwargs) for b,c,_ in zip(its,ctxs,range(max_n))]
    plt.tight_layout()
    return ctxs

# Cell
def get_grid(n, rows=None, cols=None, add_vert=0, figsize=None, double=False, title=None, return_fig=False):
    rows = rows or int(np.ceil(math.sqrt(n)))
    cols = cols or int(np.ceil(n/rows))
    if double: cols*=2 ; n*=2
    figsize = (cols*3, rows*3+add_vert) if figsize is None else figsize
    fig,axs = subplots(rows, cols, figsize=figsize)
    axs = [ax if i<n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]
    if title is not None: fig.suptitle(title, weight='bold', size=14)
    return (fig,axs) if return_fig else axs

# Cell
@typedispatch
def show_batch(x:TensorTS, y, its, ctxs=None, max_n=9, rows=None, cols=None, figsize=None, title=None, **kwargs):
    if ctxs is None: ctxs = get_grid(max_n, rows=rows, cols=cols, figsize=figsize)
    # if ctxs is None: ctxs = Inf.nones

    # Original
    ctxs = default_show_batch(x, y, its, ctxs=ctxs, max_n=max_n, **kwargs)
    # ctxs = show_batch[object](x, y, its, ctxs=ctxs, max_n=max_n, **kwargs)
    if title:
        plt.suptitle(title, fontsize=16)
        plt.subplots_adjust()
        plt.subplots_adjust(left=0.0, wspace=0.4, top=0.9, bottom=0.5)
    return ctxs

# Cell
@typedispatch
#def show_results(x:TensorImage, y, samples, outs, ctxs=None, max_n=9, rows=None, cols=None, figsize=None, **kwargs):
def show_results(x:TensorTS, y, its,  outs, ctxs=None, max_n=9, rows=None, cols=None, figsize=None, **kwargs):
    # if ctxs is None: ctxs = get_grid(min(len(its), max_n), rows=rows, cols=cols, add_vert=1, figsize=figsize)
    s = len(its)  # min(len(samples), max_n)
    # max_n = min(s, max_n)
    if ctxs is None: ctxs = get_grid(max_n, rows=rows, cols=cols, add_vert=1, figsize=figsize)
    # print(len(its), max_n)
    # print(its)
    # print(type(y))
    ctxs = [b[0].show(ctx=c, title=f'{o} / {b[1]}', **kwargs) for b,o,c,_ in zip(its,outs,ctxs,range(max_n))]
    # if title:
    #     plt.suptitle(title, fontsize=16)
    #     plt.subplots_adjust(left=0.0, wspace=0.4, top=0.9)
    plt.tight_layout()
    return ctxs
