# AUTOGENERATED! DO NOT EDIT! File to edit: 81_timeseries_core.ipynb (unless otherwise specified).

__all__ = ['TensorTS', 'ToTensorTSBlock', 'ToTensorTS', 'LabelTS', 'get_stats', 'Normalize', 'SelfNormalize',
           'default_show_batch']

# Cell
from fastai2.torch_basics import *
# from fastai2.test import *
from fastai2.data.all import *

# Cell
class TensorTS(TensorBase):
    "Transform a 2D array into a Tensor"
#     def __rep__(): return f"{self.__class__.__name__}\n {self.shape}"
    def show(self, ctx=None, title=None, chs=None, leg=True, **kwargs):
        "Display timeseries plots for all selected channels list `chs`"
        if ctx is None: fig, ctx = plt.subplots()
        t = range(self.shape[1])
        chs_max = max(chs) if chs else 0
        channels = chs if (chs and (chs_max < self.shape[0])) else range(self.shape[0])
        for ch in channels:
            ctx.plot(t, self[ch], label='ch'+str(ch))
        if leg: ctx.legend(loc='upper right', ncol=2, framealpha=0.5)
        if title: ctx.set_title(title)

# Cell
class ToTensorTSBlock(Transform):
    def encodes(self, x):
        return TensorTS(x)

# Cell
class ToTensorTS(Transform):
    # x : tuple representing (2D array, Label)
    def encodes(self, x):
        return TensorTS(x[0])

# Cell
class LabelTS(Transform):
    def encodes(self, x):
        return x[1]

# Cell
def get_stats(train, scale_type='standardize', scale_subtype='all_samples'):
    if scale_type == 'normalize':
        if scale_subtype == 'all_samples':
            train_min = train.min(keepdims=True)
            train_max = train.max(keepdims=True)
        elif scale_subtype == 'per_sample':
            train_min = train.min(axis=(1, 2), keepdims=True)
            train_max = train.max(axis=(1, 2), keepdims=True)
        elif scale_subtype == 'per_channel':
            train_min = train.min(axis=(0, 2), keepdims=True)
            train_max = train.max(axis=(0, 2), keepdims=True)
        elif scale_subtype == 'per_sample_per_channel':
            train_min = train.min(axis=(2), keepdims=True)
            train_max = train.max(axis=(2), keepdims=True)
        else:
            print('***** Please, select a valid  scaling_subtype *****')
            return
        min, max = train_min, train_max
        return min, max

    elif scale_type == 'standardize':
        print(scale_type)
        if scale_subtype == 'all_samples':
            train_mean = train.mean(keepdims=True)
            train_std = train.std(keepdims=True)
        elif scale_subtype == 'per_sample':
            train_mean = train.mean(axis=(1, 2), keepdims=True)
            train_std = train.std(axis=(1, 2), keepdims=True)
        elif scale_subtype == 'per_channel':
            train_mean = train.mean(axis=(0, 2), keepdims=True)
            train_std = train.std(axis=(0, 2), keepdims=True)
        elif scale_subtype == 'per_sample_per_channel':
            train_mean = train.mean(axis=(2), keepdims=True)
            train_std = train.std(axis=(2), keepdims=True)
        else:
            print('***** Please, select a valid  scaling_subtype *****')
            return
        mean, std = torch.tensor(train_mean), torch.tensor(train_std)
        return mean, std

    else:
        print('***** Please, select a valid  scaling_type *****')
        return

# Cell
@docs
class Normalize(Transform):
    "Normalize/denorm batch of `TimeseriesTensor`"
    order=99
    def __init__(self, mean, std, cuda=True):
        f = to_device if cuda else noop
        self.mean,self.std = f(mean), f(std)

    def encodes(self, x:TensorTS):
        print('Normalize - encodes')
        return (x-self.mean) / self.std
    def decodes(self, x:TensorTS):
        f = to_cpu if x.device.type=='cpu' else noop
        return (x*f(self.std) + f(self.mean))

    _docs=dict(encodes="Normalize batch", decodes="Denormalize batch")

# Cell
@docs
class SelfNormalize(Transform):
    "Normalize/denorm a `TimeseriesTensor` either per sample or per channel "
    order=99
    def __init__(self,  scale_type='standardize', scale_subtype='per_sample', scale_range=(-1, 1)):
        self.scale_type = scale_type
        self.scale_subtype = scale_subtype
        self.scale_range = scale_range


    def encodes(self, x:TensorTS):
#         print('normalize')
        if scale_type is None:
            return x

        if self.scale_type == 'normalize':
#             print('normalize')
            if self.scale_subtype == 'per_sample':
                self.min = (torch.min(x)).expand_as(x)
                self.max = (torch.max(x)).expand_as(x)
#                 print(self.min)
#                 print(self.max)
            elif self.scale_subtype == 'per_sample_per_channel':
#                 print('per_sample_per_channel')
                self.min = (torch.min(t, dim=1, keepdims=True).values).expand_as(x)
                self.max = (torch.max(t, dim=1, keepdims=True).values).expand_as(x)
#                 print(self.min)
#                 print(self.max)
            else: return x
            return ((x-self.min) / (self.max - self.min)) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]
        elif self.scale_type == 'standardize':
#             print('standardize')
            if self.scale_subtype == 'per_sample':
                self.mean = x.mean(axis=(0,1), keepdims=True)
                self.std = x.std(axis=(0,1), keepdims=True)
            elif self.scale_subtype == 'per_sample_per_channel':
                print('per_sample_per_channel')
#                 self.mean = x.mean(axis=(1), keepdims=True)
                self.std = x.std(axis=(1), keepdims=True)
            else: return x
            return (x-self.mean) / self.std
        else: return x
#     def decodes(self, x:TensorTS):
#         f = to_cpu if x.device.type=='cpu' else noop
#         return (x*f(self.std) + f(self.mean))

    _docs=dict(encodes="Normalize either per sample or per channel") #, decodes="Denormalize either per sample or per channel"

# Cell
def default_show_batch(x, y, its, ctxs=None, max_n=9, **kwargs):
    if ctxs is None: ctxs = Inf.nones
    ctxs = [b[0].show(ctx=c, title=b[1], **kwargs) for b,c,_ in zip(its,ctxs,range(max_n))]
    plt.tight_layout()
    return ctxs

# Cell
@typedispatch
def show_batch(x:TensorTS, y, its, ctxs=None, max_n=9, rows=None, cols=None, figsize=None, title=None, **kwargs):
    if ctxs is None: ctxs = get_grid(max_n, rows=rows, cols=cols, figsize=figsize)
    # if ctxs is None: ctxs = Inf.nones

    # Original
    ctxs = default_show_batch(x, y, its, ctxs=ctxs, max_n=max_n, **kwargs)
    # ctxs = show_batch[object](x, y, its, ctxs=ctxs, max_n=max_n, **kwargs)
    if title:
        plt.suptitle(title, fontsize=16)
        plt.subplots_adjust()
        plt.subplots_adjust(left=0.0, wspace=0.4, top=0.9, bottom=0.5)
    return ctxs